{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iv0uE0sI5H48"
      },
      "outputs": [],
      "source": [
        "!pip install -U aiogram\n",
        "!pip install -U aiogram openai nest_asyncio\n",
        "!pip install openai\n",
        "!pip install -U google-generativeai\n",
        "import asyncio\n",
        "import openai\n",
        "from aiogram import Bot, Dispatcher, types\n",
        "from aiogram.filters import Command\n",
        "from aiogram.types import Message\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "from google.colab import userdata\n",
        "import nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZLS7IfK6Iet"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['TELEGRAM_BOT_TOKEN'] = userdata.get('TELEGRAM_BOT_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obNRAeWZ6MJM"
      },
      "outputs": [],
      "source": [
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WI5xMIU6-n7"
      },
      "outputs": [],
      "source": [
        "user_conversations = {}\n",
        "MAX_HISTORY_LENGTH = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByyAaQEjY4mW"
      },
      "outputs": [],
      "source": [
        "dp = Dispatcher()\n",
        "\n",
        "active_users = set()\n",
        "\n",
        "@dp.message(Command(\"start\"))\n",
        "async def cmd_start(message: Message):\n",
        "    await message.reply(\"Hi\\nI am Your Buddy.\\nDeveloped by Sushean\")\n",
        "\n",
        "\n",
        "@dp.message(Command(\"help\"))\n",
        "async def cmd_help(message: Message):\n",
        "    help_text = (\n",
        "        \"ðŸ†˜ *Help Menu*\\n\\n\"\n",
        "        \"/start - Start the bot\\n\"\n",
        "        \"/help - Show this help message\\n\"\n",
        "        \"/StartChat - Start GPT chat mode\\n\"\n",
        "        \"/close - End GPT chat mode\\n\"\n",
        "        \"Use Imagine Keyword at start of chat to Genegate images \\n\"\n",
        "        \"Type anything else to talk to the bot (when in chat mode)!\"\n",
        "    )\n",
        "    await message.reply(help_text, parse_mode=\"Markdown\")\n",
        "\n",
        "@dp.message(Command(\"StartChat\"))\n",
        "async def cmd_start_chat(message: Message):\n",
        "    user_id = message.from_user.id\n",
        "    active_users.add(user_id)\n",
        "    user_conversations[user_id] = []  # Start fresh conversation memory\n",
        "    await message.reply(\"âœ… Chat mode activated. Send me anything and Iâ€™ll respond!\\nSend /close to stop.\")\n",
        "\n",
        "@dp.message(Command(\"close\"))\n",
        "async def cmd_close_chat(message: Message):\n",
        "    user_id = message.from_user.id\n",
        "    active_users.discard(user_id)\n",
        "    user_conversations.pop(user_id, None)  # Clear memory\n",
        "    await message.reply(\"âŒ Chat mode deactivated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNcdbm4VhZYR"
      },
      "outputs": [],
      "source": [
        "@dp.message(lambda msg: msg.text.lower().startswith(\"imagine\"))\n",
        "async def handle_imagine(message: Message):\n",
        "    user_id = message.from_user.id\n",
        "    if user_id in active_users:\n",
        "        try:\n",
        "            prompt = message.text[len(\"imagine\"):].strip()\n",
        "            if not prompt:\n",
        "                await message.reply(\"âš ï¸ Please provide a description after 'imagine'.\")\n",
        "                return\n",
        "\n",
        "            await message.reply(\"ðŸ§  Generating image... please wait â³\")\n",
        "\n",
        "            response = openai.images.generate(\n",
        "                model=\"dall-e-3\",  \n",
        "                prompt=prompt,\n",
        "                n=1,\n",
        "                size=\"1024x1024\",  \n",
        "                quality=\"standard\"  \n",
        "            )\n",
        "\n",
        "            image_url = response.data[0].url\n",
        "            await message.reply_photo(photo=image_url, caption=f\"ðŸ–¼ï¸ *Here's your image for:* `{prompt}`\", parse_mode=\"Markdown\")\n",
        "\n",
        "        except Exception as e:\n",
        "            await message.reply(f\"âŒ Failed to generate image:\\n`{str(e)}`\", parse_mode=\"Markdown\")\n",
        "    else:\n",
        "        await message.reply(\"âš ï¸ You need to start chat mode first using /StartChat.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZApdjBemDxOG"
      },
      "outputs": [],
      "source": [
        "@dp.message(lambda msg: msg.photo)\n",
        "async def handle_photo(message: Message):\n",
        "    user_id = message.from_user.id\n",
        "    if user_id in active_users:\n",
        "        try:\n",
        "            photo = message.photo[-1]\n",
        "            file = await message.bot.get_file(photo.file_id)\n",
        "            file_path = file.file_path\n",
        "            file_url = f\"https://api.telegram.org/file/bot{os.environ['TELEGRAM_BOT_TOKEN']}/{file_path}\"\n",
        "\n",
        "            # Ensure content is always a valid string\n",
        "            prompt_text = message.caption if message.caption else \"Describe this image.\"\n",
        "\n",
        "            response = openai.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": prompt_text},\n",
        "                        {\"type\": \"image_url\", \"image_url\": {\"url\": file_url}}\n",
        "                    ]}\n",
        "                ],\n",
        "                max_tokens=200\n",
        "            )\n",
        "\n",
        "            await message.reply(response.choices[0].message.content.strip(), parse_mode=\"Markdown\")\n",
        "\n",
        "        except Exception as e:\n",
        "            await message.reply(f\"âš ï¸ Error handling photo:\\n{e}{file_url}\")\n",
        "    else:\n",
        "        await message.reply(\"ðŸ¤– Not in chat mode.\\nUse /StartChat to begin.\")\n",
        "\n",
        "@dp.message(lambda msg: msg.voice)\n",
        "async def handle_voice(message: Message):\n",
        "    user_id = message.from_user.id\n",
        "    if user_id in active_users:\n",
        "        try:\n",
        "            voice = message.voice\n",
        "            file = await message.bot.get_file(voice.file_id)\n",
        "            file_path = file.file_path\n",
        "            file_url = f\"https://api.telegram.org/file/bot{os.environ['TELEGRAM_BOT_TOKEN']}/{file_path}\"\n",
        "\n",
        "            import requests\n",
        "            import tempfile\n",
        "\n",
        "            r = requests.get(file_url)\n",
        "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".oga\") as temp_audio:\n",
        "                temp_audio.write(r.content)\n",
        "                temp_audio.flush()\n",
        "\n",
        "                audio_file = open(temp_audio.name, \"rb\")\n",
        "                transcript = openai.audio.transcriptions.create(\n",
        "                    model=\"whisper-1\",\n",
        "                    file=audio_file\n",
        "                )\n",
        "                text = transcript.text\n",
        "\n",
        "            await message.reply(f\"ðŸ—£ You said: {text}\")\n",
        "\n",
        "            # Optional: GPT reply\n",
        "            response = openai.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": text}\n",
        "                ]\n",
        "            )\n",
        "            await message.reply(response.choices[0].message.content.strip())\n",
        "\n",
        "        except Exception as e:\n",
        "            await message.reply(f\"âš ï¸ Error handling audio: {e}\")\n",
        "    else:\n",
        "        await message.reply(\"ðŸ¤– Not in chat mode.\\nUse /StartChat to begin.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4Qmn-OILe-i"
      },
      "outputs": [],
      "source": [
        "@dp.message()\n",
        "async def handle_message(message: Message):\n",
        "    user_id = message.from_user.id\n",
        "\n",
        "    if user_id not in active_users:\n",
        "        await message.reply(\"ðŸ¤– Not in chat mode.\\nUse /StartChat to begin.\")\n",
        "        return\n",
        "\n",
        "    if not message.text:\n",
        "        await message.reply(\"âš  Please send a text message.\")\n",
        "        return\n",
        "\n",
        "    prompt = message.text\n",
        "\n",
        "    try:\n",
        "\n",
        "        history = user_conversations.get(user_id, [])\n",
        "        history.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "        # Keep only the last N messages for memory\n",
        "        if len(history) > MAX_HISTORY_LENGTH:\n",
        "            history = history[-MAX_HISTORY_LENGTH:]\n",
        "\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"system\", \"content\": \"You're a helpful assistant.\"}] + history,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "        reply = response.choices[0].message.content.strip()\n",
        "\n",
        "        history.append({\"role\": \"assistant\", \"content\": reply})\n",
        "        user_conversations[user_id] = history  # Save updated memory\n",
        "\n",
        "        await message.reply(reply, parse_mode=\"Markdown\")\n",
        "\n",
        "    except Exception as e:\n",
        "        await message.reply(f\"âš  Error while generating response.\\n{e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWE_NWX88v14"
      },
      "outputs": [],
      "source": [
        "async def main():\n",
        "  bot = Bot(os.environ['TELEGRAM_BOT_TOKEN'])\n",
        "  await dp.start_polling(bot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfhZMPNc9CVg"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import asyncio\n",
        "    logging.basicConfig(level=logging.INFO, stream=sys.stdout)\n",
        "    asyncio.run(main())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
